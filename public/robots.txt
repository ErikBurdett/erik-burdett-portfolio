# robots.txt for erikburdett.dev
# https://www.robotstxt.org/

# Allow legitimate search engine crawlers for SEO
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: DuckDuckBot
Allow: /

User-agent: Slurp
Allow: /

User-agent: facebot
Allow: /

User-agent: LinkedInBot
Allow: /

User-agent: Twitterbot
Allow: /

# Block AI scrapers and data harvesters
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web
Disallow: /

User-agent: Bytespider
Disallow: /

User-agent: PetalBot
Disallow: /

User-agent: Sogou
Disallow: /

# Block aggressive crawlers and scrapers
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: BLEXBot
Disallow: /

User-agent: DataForSeoBot
Disallow: /

User-agent: serpstatbot
Disallow: /

# Block known bad bots and scrapers
User-agent: MegaIndex
Disallow: /

User-agent: Yandex
Disallow: /

User-agent: Baiduspider
Disallow: /

User-agent: YisouSpider
Disallow: /

User-agent: Applebot
Disallow: /

User-agent: SEOkicks
Disallow: /

User-agent: WebDataStats
Disallow: /

User-agent: Screaming Frog SEO Spider
Disallow: /

User-agent: Mail.RU_Bot
Disallow: /

User-agent: magpie-crawler
Disallow: /

User-agent: Nutch
Disallow: /

User-agent: AspiegelBot
Disallow: /

User-agent: Amazonbot
Disallow: /

User-agent: omgili
Disallow: /

User-agent: omgilibot
Disallow: /

# Default rule for all other bots - be cautious
User-agent: *
Allow: /
Crawl-delay: 10

# Sitemap location
Sitemap: https://erikburdett.dev/sitemap.xml

